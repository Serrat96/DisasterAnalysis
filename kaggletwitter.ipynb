{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "CbnCtyv1VsYE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "4V8gPAzuVsYH"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"train.csv\")\n",
    "X_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HqVRvh7_VsYI",
    "outputId": "3a4db2ec-cfe4-4f8f-eb49-d3f46f924efe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "4  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DZARCZUVsYJ",
    "outputId": "003a529c-c5c9-4626-b7c2-5b031663eff7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       #RockyFire Update => California Hwy. 20 closed...\n",
       "4       I'm on top of the hill and I can see a fire in...\n",
       "                              ...                        \n",
       "5341    Suicide bomber kills 15 in Saudi security site...\n",
       "5342    Two giant cranes holding a bridge collapse int...\n",
       "5343    @aria_ahrary @TheTawniest The out of control w...\n",
       "5344    Police investigating after an e-bike collided ...\n",
       "5345    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 2291, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['target']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQbgZrXhVsYK",
    "outputId": "a29a42dd-cb9b-46d0-f382-2e6976f2607f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8                                          What's up man?\n",
       "9                                           I love fruits\n",
       "10                                       Summer is lovely\n",
       "11                                      My car is so fast\n",
       "12                                 this is ridiculous....\n",
       "                              ...                        \n",
       "5325    @widda16 ... He's gone. You can relax. I thoug...\n",
       "5326     @jt_ruff23 @cameronhacker and I wrecked you both\n",
       "5327    Three days off from work and they've pretty mu...\n",
       "5328    @engineshed Great atmosphere at the British Li...\n",
       "5329    Cramer: Iger's 3 words that wrecked Disney's s...\n",
       "Name: text, Length: 3055, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['target']==0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3CwMx-JVsYK",
    "outputId": "7339c528-6088-4eee-e4aa-33895f1f62eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(sin procesar):  (5346, 5)\n",
      "X_test.shape(sin procesar):  (2267, 4)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape(sin procesar): ', X_train.shape)\n",
    "print('X_test.shape(sin procesar): ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4WxVtSUVsYL",
    "outputId": "f891d2db-1965-4a67-ef06-049151099a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape(sin duplicados):  (5286, 5)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos los duplicados\n",
    "X_train = X_train.drop_duplicates(subset='text')\n",
    "print('X_train.shape(sin duplicados): ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzFsuxoltUN7",
    "outputId": "80251433-6bda-431a-c1f2-f8a04816d1e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     people receive #wildfires evacuation orders i...\n",
       "1    just got sent this photo from ruby #alaska as ...\n",
       "2    #flood #disaster heavy rain causes flash flood...\n",
       "3    there's an emergency evacuation happening now ...\n",
       "4    i'm afraid that the tornado is coming to our area\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "signos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\>)|(\\=)|(\\<)\")\n",
    "signos_arroba = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\>)|(\\=)|(\\<)|(\\@)\")\n",
    "\n",
    "def signs_tweets(tweet):\n",
    "    return signos.sub('', tweet.lower())\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(signs_tweets)\n",
    "X_train['text'].head()\n",
    "X_test['text'] = X_test['text'].apply(signs_tweets)\n",
    "X_test['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "ldH7sKX0VsYM"
   },
   "outputs": [],
   "source": [
    "def remove_links(df):\n",
    "    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(remove_links)\n",
    "X_test['text'] = X_test['text'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "C3v0AEU8tnII",
    "outputId": "a242bcb9-bd2c-4631-c013-77573b56da56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive #wildfires evacuation orders ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster heavy rain causes flash flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there's emergency evacuation happening buildin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i'm afraid tornado coming area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   6     NaN      NaN  people receive #wildfires evacuation orders ca...\n",
       "1   7     NaN      NaN  got sent photo ruby #alaska smoke #wildfires p...\n",
       "2  10     NaN      NaN  #flood #disaster heavy rain causes flash flood...\n",
       "3  14     NaN      NaN  there's emergency evacuation happening buildin...\n",
       "4  15     NaN      NaN                     i'm afraid tornado coming area"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    return \" \".join([word for word in df.split() if word not in english_stopwords])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(remove_stopwords)\n",
    "X_train.head()\n",
    "X_test['text'] = X_test['text'].apply(remove_stopwords)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "OMxzYszzVsYO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def english_stemmer(x):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return ' '.join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(english_stemmer)\n",
    "X_test['text'] = X_test['text'].apply(english_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "vrEtB_cjVsYP"
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "fnXqVO-A2pZK",
    "outputId": "5b6f9964-2146-4068-c088-1e7436b80e69"
   },
   "outputs": [],
   "source": [
    "\"\"\"vectorizer = CountVectorizer(binary=True)\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\"\"\"\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "rMX9zklky3Vu"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', SVC(probability=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "XgBucXedVsYQ"
   },
   "outputs": [],
   "source": [
    "# Aqui definimos el espacio de par√°metros a explorar\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 1.9),\n",
    "    'vect__min_df': (10, 20,50),\n",
    "    'vect__max_features': (500, 1000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.2, 0.5, 0.7),\n",
    "    'cls__degree': [2,3,4,5],\n",
    "    'cls__gamma': [0.001, 0.1, \"auto\", 1.0, 10.0, 30.0]\n",
    "}\n",
    "\n",
    "parameters_low = {\n",
    "    'vect__max_df': (0.05, 0.06),\n",
    "    'vect__min_df': (0.05, 0.04),\n",
    "    'vect__max_features': (550, 740),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigramas or bigramas\n",
    "    'cls__C': (0.07, 0.1),\n",
    "    'cls__degree': [0.1, 0.2],\n",
    "    'cls__gamma': ['auto', 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                          parameters_low,\n",
    "                          cv=5,\n",
    "                          n_jobs=-2,\n",
    "                          scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Ok27jJGfVsYR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('cls', SVC(probability=True))]),\n",
       "             n_jobs=-2,\n",
       "             param_grid={'cls__C': (0.07, 0.1), 'cls__degree': [0.1, 0.2],\n",
       "                         'cls__gamma': ['auto', 0.1],\n",
       "                         'vect__max_df': (0.05, 0.06),\n",
       "                         'vect__max_features': (550, 740),\n",
       "                         'vect__min_df': (0.05, 0.04),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train['text'], X_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "uLQk3r1SVsYR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'cls__C': 0.07, 'cls__degree': 0.1, 'cls__gamma': 'auto', 'vect__max_df': 0.06, 'vect__max_features': 550, 'vect__min_df': 0.04, 'vect__ngram_range': (1, 1)}\n",
      "Best acc: 0.5201917199073144\n",
      "Best model: Pipeline(steps=[('vect',\n",
      "                 CountVectorizer(max_df=0.06, max_features=550, min_df=0.04)),\n",
      "                ('cls',\n",
      "                 SVC(C=0.07, degree=0.1, gamma='auto', probability=True))])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best acc:\", grid_search.best_score_)\n",
    "print(\"Best model:\", grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "zXzZWL7XvKc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba = grid_search.predict_proba(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58746351, 0.41253649],\n",
       "       [0.58746351, 0.41253649],\n",
       "       [0.58746351, 0.41253649],\n",
       "       ...,\n",
       "       [0.58746351, 0.41253649],\n",
       "       [0.58746351, 0.41253649],\n",
       "       [0.58746351, 0.41253649]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2267, 2)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-2328126d8665>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_submission['target'] = predict_proba[:,-1]\n"
     ]
    }
   ],
   "source": [
    "df_submission = X_test[['id']]\n",
    "df_submission['target'] = predict_proba[:,-1]\n",
    "df_submission.to_csv('data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "baseline_vectorizer.fit(X_train)\n",
    "\n",
    "X_baseline = baseline_vectorizer.transform(X_train)\n",
    "X_test_baseline = baseline_vectorizer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index dimension must be <= 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-3baf63d4b646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogisticRegr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlogisticRegr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \"\"\"\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Dispatch to specialized methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py\u001b[0m in \u001b[0;36m_asindices\u001b[1;34m(self, idx, length)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Index dimension must be <= 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Index dimension must be <= 2"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "\n",
    "logisticRegr.fit(X_train['text'], X_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kaggletwitter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
